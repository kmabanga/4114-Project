{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Human Activity Recognition (HAR) with MotionSense Dataset (CNN Approach.v2)"
      ],
      "metadata": {
        "id": "usr9kgGNBHIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "YJn3olJLbnub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.is_available():\n",
        "#   Returns True if a CUDA-compatible GPU is available\n",
        "# torch.device(...):\n",
        "#   Sets the computation device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Prints which device is being used\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "elysQX7EbqDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Mounts the Google Drive to the specified directory\n",
        "drive.mount('/content/drive')\n",
        "# Defines a string variable DATA_PATH_PREFIX that stores the path to a specific folder in the Drive\n",
        "# Base path to access data files\n",
        "DATA_PATH_PREFIX = \"/content/drive/MyDrive/EEE4114F - ML/\""
      ],
      "metadata": {
        "id": "O9QR3lWcbr5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ds_infos():\n",
        "    \"\"\"\n",
        "    Read the file includes data subject information.\n",
        "    Returns:\n",
        "        A pandas DataFrame that contains information about data subjects' attributes\n",
        "    \"\"\"\n",
        "    # try block to handle errors\n",
        "    try:\n",
        "        # Attempts to load a CSV file named data_subjects_info.csv from the given DATA_PATH_PREFIX path\n",
        "        # dss holds the loaded DataFrame\n",
        "        dss = pd.read_csv(DATA_PATH_PREFIX + \"data_subjects_info.csv\")\n",
        "        # Prints a success message if the file loads without issues\n",
        "        print(\"[INFO] -- Data subjects' information is imported.\")\n",
        "    # Catches the error if the file is not found at the specific path\n",
        "    except FileNotFoundError:\n",
        "        # Prints an error message\n",
        "        print(f\"[ERROR] -- data_subjects_info.csv not found at {DATA_PATH_PREFIX + 'data_subjects_info.csv'}\")\n",
        "        print(\"Please ensure DATA_PATH_PREFIX is set correctly and the file exists.\")\n",
        "        # If the file is not found, it returns an empty DataFrame with a single column named 'code'\n",
        "        return pd.DataFrame(columns=['code'])\n",
        "    # if the file was successfully read, the loaded DataFrame (stored in dss) is returned\n",
        "    return dss"
      ],
      "metadata": {
        "id": "bB_tvYRsbyu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_data_types(data_types=[\"userAcceleration\"]):\n",
        "    \"\"\"\n",
        "    Select the sensors and return a list of lists of column names.\n",
        "    Args:\n",
        "        data_types: A list of sensor data type from this list: [attitude, gravity, rotationRate, userAcceleration]\n",
        "    Returns:\n",
        "        A list of lists, where each inner list contains the column names for a sensor type (e.g., [t+\".x\",t+\".y\",t+\".z\"]).\n",
        "    \"\"\"\n",
        "    # Iintializes an empty list to store the results\n",
        "    dt_list_groups = []\n",
        "    # Loop through each sensor type specified in the data_types list\n",
        "    for t in data_types:\n",
        "        # Check if the sensor type is not attitude\n",
        "        # Yes: Construst a list of the 3-axis column names and append it to the result list\n",
        "        if t != \"attitude\":\n",
        "            dt_list_groups.append([t+\".x\", t+\".y\", t+\".z\"])\n",
        "        else:\n",
        "            # Attitude uses orientation angle\n",
        "            dt_list_groups.append([t+\".roll\", t+\".pitch\", t+\".yaw\"])\n",
        "    # Returns the list of column names\n",
        "    return dt_list_groups"
      ],
      "metadata": {
        "id": "BLeIPQIrb0mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_time_series(dt_list_groups_config, act_labels_config, trial_codes_config_map, mode=\"raw\", labeled=True):\n",
        "    \"\"\"\n",
        "    Creates a time-series DataFrame from raw sensor data files.\n",
        "    Args:\n",
        "        dt_list_groups_config: List of lists of feature column names, grouped by sensor type.\n",
        "        act_labels_config: List of activity names.\n",
        "        trial_codes_config_map: Dictionary mapping activity names to lists of trial codes.\n",
        "        mode: \"raw\" (magnitude mode is not fully supported by this simplified version).\n",
        "        labeled: Boolean, True if activity labels should be included.\n",
        "    Returns:\n",
        "        A tuple: (full_dataset_df, feature_cols_flat_list)\n",
        "            full_dataset_df: Pandas DataFrame with all time-series data and labels.\n",
        "            feature_cols_flat_list: Flat list of all feature column names.\n",
        "    \"\"\"\n",
        "    # Flattens the list of lists into a single list of feature column names\n",
        "    feature_cols_flat_list = [col for group in dt_list_groups_config for col in group]\n",
        "    num_feature_cols = len(feature_cols_flat_list)\n",
        "\n",
        "    # Initializes the column names for the final DataFrame\n",
        "    column_names_for_df = feature_cols_flat_list[:]\n",
        "    # Adds \"act\" column if activity labes are included\n",
        "    if labeled:\n",
        "        column_names_for_df.append(\"act\")\n",
        "\n",
        "    # Loads subject metada\n",
        "    all_trial_dfs = []\n",
        "    ds_list = get_ds_infos()\n",
        "    # Check if subject info is missing\n",
        "    # Yes: Prints an error message, aborts and returns an empty DataFrame\n",
        "    if ds_list.empty and 'code' not in ds_list.columns:\n",
        "        print(\"[ERROR] -- Cannot proceed without subject information.\")\n",
        "        return pd.DataFrame(columns=column_names_for_df), feature_cols_flat_list\n",
        "\n",
        "\n",
        "    print(\"[INFO] -- Creating Time-Series\")\n",
        "    # Loops hrough all subjects and trials\n",
        "    for sub_id in ds_list[\"code\"]:\n",
        "        for act_id, act_name in enumerate(act_labels_config):\n",
        "            # Checks if no trials are configure\n",
        "            # Yes:  Skips the activity\n",
        "            if act_name not in trial_codes_config_map:\n",
        "                print(f\"[WARNING] -- No trial codes found for activity: {act_name}. Skipping.\")\n",
        "                continue\n",
        "            for trial_code in trial_codes_config_map[act_name]:\n",
        "                # Constructs the full file path for each subject's trial\n",
        "                fname = f'{DATA_PATH_PREFIX}A_DeviceMotion_data/A_DeviceMotion_data/{act_name}_{trial_code}/sub_{int(sub_id)}.csv'\n",
        "                # try block to handle errors\n",
        "                try:\n",
        "                    # Attampts to read the CSV file\n",
        "                    raw_data_per_trial = pd.read_csv(fname)\n",
        "                # Catches the error if the file is not found\n",
        "                except FileNotFoundError:\n",
        "                    # Prints a warninng message\n",
        "                    print(f\"[WARNING] -- File not found: {fname}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Drops unwanted index columns automatically added by pandas during saving\n",
        "                raw_data_per_trial = raw_data_per_trial.drop(['Unnamed: 0'], axis=1, errors='ignore')\n",
        "\n",
        "                # Extracts only the relevant feature columns for this trial\n",
        "                current_trial_features = raw_data_per_trial[feature_cols_flat_list].values\n",
        "\n",
        "               # Checks if labeled\n",
        "               # Creates an array of labels with the same numbeer as the featires\n",
        "                if labeled:\n",
        "                    labels_for_this_trial = np.full((len(raw_data_per_trial), 1), act_id)\n",
        "                    trial_data_np = np.concatenate((current_trial_features, labels_for_this_trial), axis=1)\n",
        "                else:\n",
        "                    trial_data_np = current_trial_features\n",
        "                # Converts the current trial into a DataFrame and apends to the list of all trials\n",
        "                all_trial_dfs.append(pd.DataFrame(data=trial_data_np, columns=column_names_for_df))\n",
        "\n",
        "    # Checks if no trails loaded successfully\n",
        "    # Yes: Returns an empty DataFrame\n",
        "    if not all_trial_dfs:\n",
        "        print(\"[ERROR] -- No data successfully loaded. Please check file paths, data structure, and configurations.\")\n",
        "        return pd.DataFrame(columns=column_names_for_df), feature_cols_flat_list\n",
        "\n",
        "    print(f\"[INFO] -- Concatenating {len(all_trial_dfs)} individual trial DataFrames.\")\n",
        "    # Concatenates all DataFrames into one final DataFrame\n",
        "    full_dataset_df = pd.concat(all_trial_dfs, ignore_index=True)\n",
        "    # Returns the complete time-series dataset (full_dataset_df) and a list of column names for model input (feature_cols_flat_list)\n",
        "    return full_dataset_df, feature_cols_flat_list"
      ],
      "metadata": {
        "id": "GsK8CY-Eb146"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines a list of activity labels\n",
        "ACT_LABELS = [\"dws\",\"ups\", \"wlk\", \"jog\", \"std\", \"sit\"]\n",
        "# Creates a mapping between each activity and its corresponding trial codes\n",
        "TRIAL_CODES = {\n",
        "    ACT_LABELS[0]:[1,2,11], #dws: trials 1, 2, 11\n",
        "    ACT_LABELS[1]:[3,4,12], #ups: trials 3, 4, 12\n",
        "    ACT_LABELS[2]:[7,8,15], #wlk: trials 7, 8, 15\n",
        "    ACT_LABELS[3]:[9,16],   #jog: trials 9, 16\n",
        "    ACT_LABELS[4]:[6,14],   #std: trials 6, 14\n",
        "    ACT_LABELS[5]:[5,13]    #sit: trials 5, 13\n",
        "}"
      ],
      "metadata": {
        "id": "zovVNWM9b4Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specifies which sensor data types to include from each CSV file\n",
        "# Available options: \"attitude\", \"gravity\", \"rotationRate\", \"userAcceleration\"\n",
        "SELECTED_SENSOR_DATA_TYPES = [\"userAcceleration\", \"rotationRate\"]\n",
        "# Prints the current configuration\n",
        "print(f\"[INFO] -- Selected sensor data types: {SELECTED_SENSOR_DATA_TYPES}\")\n",
        "print(f\"[INFO] -- Selected activities: {ACT_LABELS}\")"
      ],
      "metadata": {
        "id": "31ZXV_VYcBWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sets sensor data types and groups them accordingly\n",
        "dt_list_feature_groups = set_data_types(SELECTED_SENSOR_DATA_TYPES)\n",
        "\n",
        "# Creates the time-series dataset from raw sensor data using the specified activity labels and trial codes\n",
        "dataset, feature_columns_list = create_time_series(dt_list_feature_groups, ACT_LABELS, TRIAL_CODES, mode=\"raw\", labeled=True)\n",
        "\n",
        "# Checks if the resulting dataset is empty\n",
        "# Yes: Halts the execution\n",
        "if dataset.empty:\n",
        "    print(\"[STOP] -- Dataset is empty. Halting execution.\")\n",
        "    exit()\n",
        "# No: Displays the dataset shape and preview\n",
        "else:\n",
        "    print(f\"[INFO] -- Shape of raw time-Series dataset: {dataset.shape}\")\n",
        "    print(dataset.head())\n",
        "    # Determines the number of features\n",
        "    NUM_FEATURES = len(feature_columns_list)\n",
        "    # Determines the number of activity classes\n",
        "    NUM_CLASSES = len(ACT_LABELS)\n",
        "    # Prints the number of features and class count\n",
        "    print(f\"[INFO] -- Number of features: {NUM_FEATURES}\")\n",
        "    print(f\"[INFO] -- Number of classes: {NUM_CLASSES}\")"
      ],
      "metadata": {
        "id": "XXZiUWfccInj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_windows_from_df(data_df, feature_cols, label_col, window_size, stride):\n",
        "    # Initializes lists to hold windowed feature arrays and corresponding labels\n",
        "    X_windows_list = []\n",
        "    Y_labels_list = []\n",
        "\n",
        "    # Converts feature and label columns from DataFrame to NumPy arrays\n",
        "    feature_data_np = data_df[feature_cols].values\n",
        "    label_data_np = data_df[label_col].values\n",
        "\n",
        "    # Prints debug output showing the shape of the feature matrix\n",
        "    print(f\"[DEBUG] -- Creating windows. Input feature_data_np shape: {feature_data_np.shape}\")\n",
        "\n",
        "    for i in range(0, len(feature_data_np) - window_size + 1, stride):\n",
        "        # Extracts a window of features\n",
        "        window_features = feature_data_np[i : i + window_size]\n",
        "        # Extracts a corresponding window of labels\n",
        "        window_labels_raw = label_data_np[i : i + window_size]\n",
        "        # Transpose feature window to shape (num_features, window_size)\n",
        "        X_windows_list.append(window_features.T)\n",
        "        # Determines the majority label in the window (mode of labels)\n",
        "        label_counts = np.bincount(window_labels_raw.astype(int))\n",
        "        mode_label = np.argmax(label_counts)\n",
        "        Y_labels_list.append(mode_label)\n",
        "        #  Prints progress every 5000 windows\n",
        "        if (len(X_windows_list) % 5000) == 0:\n",
        "            print(f\"[INFO] -- Processed {len(X_windows_list)} windows...\")\n",
        "    # Convert lists to NumPy arrays and return\n",
        "    return np.array(X_windows_list), np.array(Y_labels_list)"
      ],
      "metadata": {
        "id": "3JrbYwRUcNQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checks if the dataset is not empty:\n",
        "# Yes: Proceed\n",
        "if not dataset.empty:\n",
        "    # Defines the size of each window (number of time steps)\n",
        "    WINDOW_SIZE = 200\n",
        "    # Defines the stride (step size between windows)\n",
        "    STRIDE = 100\n",
        "\n",
        "    # Initialize a standard scaler for normalizing feature values (zero mean, unit variance)\n",
        "    scaler = StandardScaler()\n",
        "    # Creates a copy of the dataset to apply scaling\n",
        "    scaled_dataset = dataset.copy()\n",
        "    # Applies scaling to the selected sensor feature columns\n",
        "    scaled_dataset[feature_columns_list] = scaler.fit_transform(dataset[feature_columns_list])\n",
        "    # Prints a confirmation message\n",
        "    print(\"[INFO] -- Features scaled using StandardScaler.\")\n",
        "\n",
        "    # Starts the windowing processto cnvert time-series data into overlapping segments\n",
        "    print(\"[INFO] -- Starting windowing...\")\n",
        "    X_windowed, Y_windowed = create_windows_from_df(\n",
        "        scaled_dataset, feature_columns_list, 'act',\n",
        "        WINDOW_SIZE, STRIDE\n",
        "    )\n",
        "\n",
        "    # Prints shapes of the resulting windowed feature and label arrays\n",
        "    print(f\"[INFO] -- Shape of windowed X: {X_windowed.shape}\")\n",
        "    print(f\"[INFO] -- Shape of windowed Y: {Y_windowed.shape}\")\n",
        "\n",
        "    # First split: splits the dataset into 80% for training + validation and 20% for testing\n",
        "    X_train_val, X_test, Y_train_val, Y_test = train_test_split(\n",
        "        X_windowed, Y_windowed, test_size=0.2, random_state=42, stratify=Y_windowed\n",
        "    )\n",
        "\n",
        "    # Second split: splits the 80% into 75% for trainining and 25% for validation\n",
        "    X_train, X_val, Y_train, Y_val = train_test_split(\n",
        "        X_train_val, Y_train_val, test_size=0.25, random_state=42, stratify=Y_train_val\n",
        "    )\n",
        "\n",
        "    # Prints the shapes of all splits\n",
        "    print(f\"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}\")\n",
        "    print(f\"X_val shape: {X_val.shape}, Y_val shape: {Y_val.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}, Y_test shape: {Y_test.shape}\")\n",
        "# No: Skip\n",
        "else:\n",
        "    print(\"[INFO] -- Dataset empty, skipping preprocessing and splitting.\")"
      ],
      "metadata": {
        "id": "Fx3rgakIcRvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MotionSensePyTorchDataset(Dataset):\n",
        "    def __init__(self, X_data, Y_data):\n",
        "        # Convers input features and labels to PyTorch tensors\n",
        "        self.X = torch.tensor(X_data, dtype=torch.float32)\n",
        "        self.Y = torch.tensor(Y_data, dtype=torch.long)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Returns a single sample (X, Y) at the given index\n",
        "        return self.X[index], self.Y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples\n",
        "        return len(self.X)\n",
        "\n",
        "if not dataset.empty:\n",
        "    # Create dataset objects for training, validation, and testing\n",
        "    train_torch_dataset = MotionSensePyTorchDataset(X_train, Y_train)\n",
        "    val_torch_dataset = MotionSensePyTorchDataset(X_val, Y_val)\n",
        "    test_torch_dataset = MotionSensePyTorchDataset(X_test, Y_test)\n",
        "\n",
        "    # Set the batch size for model training and evaluation\n",
        "    BATCH_SIZE = 256\n",
        "    train_loader = DataLoader(train_torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_torch_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_loader = DataLoader(test_torch_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Prints successful DataLoader creation\n",
        "    print(\"[INFO] -- PyTorch DataLoaders created for Train, Validation, and Test sets.\")\n",
        "else:\n",
        "    print(\"[INFO] -- Dataset empty, skipping DataLoader creation.\")"
      ],
      "metadata": {
        "id": "_hG4t5frcXYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_HAR_Model(nn.Module):\n",
        "    def __init__(self, num_input_features, num_activity_classes):\n",
        "        super(CNN_HAR_Model, self).__init__()\n",
        "        # First convolutional layer\n",
        "        self.conv1 = nn.Conv1d(in_channels=num_input_features, out_channels=100, kernel_size=10, padding='same')\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Conv1d(in_channels=100, out_channels=100, kernel_size=10, padding='same')\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        # Max pooling reduces temporal resolution by selecting the maximum in windows of size 3\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=3)\n",
        "\n",
        "        # Third convolutional layer\n",
        "        self.conv3 = nn.Conv1d(in_channels=100, out_channels=160, kernel_size=10, padding='same')\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        # Fourth convolutional layer\n",
        "        self.conv4 = nn.Conv1d(in_channels=160, out_channels=160, kernel_size=10, padding='same')\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "        # Global average pooling reduces each feature map to a single value\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Flattens the output for the fully connected layer\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Dropout for regularization to prevent overfitting\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # Final fully connected layer that outputs logits for classification\n",
        "        self.fc_out = nn.Linear(160, num_activity_classes)\n",
        "\n",
        "    def forward(self, x_input):\n",
        "        # Forward pass through the network\n",
        "        x = self.relu1(self.conv1(x_input))\n",
        "        x = self.relu2(self.conv2(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.relu3(self.conv3(x))\n",
        "        x = self.relu4(self.conv4(x))\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout(x)\n",
        "        output_logits = self.fc_out(x)\n",
        "        return output_logits\n",
        "\n",
        "    def forward(self, x_input):\n",
        "        # Forward pass through the network\n",
        "        x = self.relu1(self.conv1(x_input))\n",
        "        x = self.relu2(self.conv2(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.relu3(self.conv3(x))\n",
        "        x = self.relu4(self.conv4(x))\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout(x)\n",
        "        output_logits = self.fc_out(x)\n",
        "        return output_logits"
      ],
      "metadata": {
        "id": "7lYwqEagccvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not dataset.empty:\n",
        "    # Instantiate the CNN model with the number of input features and number of activity classes\n",
        "    # Move the model to the specified device\n",
        "    model = CNN_HAR_Model(num_input_features=NUM_FEATURES, num_activity_classes=NUM_CLASSES).to(device)\n",
        "    # Prints confimation output\n",
        "    print(\"[INFO] -- CNN Model instantiated:\")\n",
        "    # Prints number of features (channels for 1D CNN)\n",
        "    print(f\"[INFO] -- Input features (channels for 1D CNN): {NUM_FEATURES}\")\n",
        "    # Prints the structure and parameter count of the model\n",
        "    print(model)\n",
        "\n",
        "    # Defines the loss function â†’ CrossEntropyLoss\n",
        "    loss_criterion = nn.CrossEntropyLoss()\n",
        "    # Defines the optimizer (Adam) with a learning rate of 0.001\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "else:\n",
        "    print(\"[INFO] -- Dataset empty, skipping model instantiation.\")"
      ],
      "metadata": {
        "id": "Ap0Y77TMckky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not dataset.empty:\n",
        "    # Defines the number of epochs\n",
        "    NUM_EPOCHS = 50\n",
        "    print(f\"[INFO] -- Starting training for {NUM_EPOCHS} epochs...\")\n",
        "    # Lists to store training/validation loss and accuracy for each epoch\n",
        "    train_losses_history = []\n",
        "    val_losses_history = []\n",
        "    val_accuracies_history = []\n",
        "\n",
        "    # Training loop over the number of epochs\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # Sets the model to training mode\n",
        "        model.train()\n",
        "        # Accumulator for training loss in this epoch\n",
        "        current_epoch_train_loss = 0\n",
        "        # Iterates over batches of the training DataLoader\n",
        "        for batch_idx, (data_batch, labels_batch) in enumerate(train_loader):\n",
        "            # Moves data and labels to the same device as the model\n",
        "            data_batch, labels_batch = data_batch.to(device), labels_batch.to(device)\n",
        "            # Resets gradients from previous step\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            outputs_logits = model(data_batch)\n",
        "            # Computes loss\n",
        "            loss = loss_criterion(outputs_logits, labels_batch)\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            # Updates model parameters\n",
        "            optimizer.step()\n",
        "            # Accumulates batch loss\n",
        "            current_epoch_train_loss += loss.item()\n",
        "            # Prints training progress every 50 batches\n",
        "            if (batch_idx + 1) % 50 == 0:\n",
        "                 print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Batch [{batch_idx+1}/{len(train_loader)}], Train Batch Loss: {loss.item():.4f}\")\n",
        "        # Computes average training loss for the epoch and stores it\n",
        "        avg_epoch_train_loss = current_epoch_train_loss / len(train_loader)\n",
        "        train_losses_history.append(avg_epoch_train_loss)\n",
        "\n",
        "        # Evaluation on validation set\n",
        "        # Sets model to evaluation mode\n",
        "        model.eval()\n",
        "        current_epoch_val_loss = 0\n",
        "        total_val_correct = 0\n",
        "        total_val_samples = 0\n",
        "        # Disables gradient computation for validation\n",
        "        with torch.no_grad():\n",
        "            for data_batch, labels_batch in val_loader:\n",
        "                # Moves validation data to appropriate device\n",
        "                data_batch, labels_batch = data_batch.to(device), labels_batch.to(device)\n",
        "                # Forward pass\n",
        "                outputs_logits = model(data_batch)\n",
        "                 # Computes validation loss\n",
        "                loss = loss_criterion(outputs_logits, labels_batch)\n",
        "                current_epoch_val_loss += loss.item()\n",
        "                # Gets predicted class indices\n",
        "                _, predicted_labels = torch.max(outputs_logits.data, 1)\n",
        "                 # Counts correctly predicted samples\n",
        "                total_val_samples += labels_batch.size(0)\n",
        "                total_val_correct += (predicted_labels == labels_batch).sum().item()\n",
        "        # Computes average validation loss and accuracy\n",
        "        avg_epoch_val_loss = current_epoch_val_loss / len(val_loader)\n",
        "        epoch_val_accuracy = 100 * total_val_correct / total_val_samples\n",
        "        val_losses_history.append(avg_epoch_val_loss)\n",
        "        val_accuracies_history.append(epoch_val_accuracy)\n",
        "\n",
        "        # Prints epoch summary\n",
        "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {avg_epoch_train_loss:.4f}, Val Loss: {avg_epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.2f}%\")\n",
        "    # Training complete\n",
        "    print(\"[INFO] -- Training Finished.\")\n",
        "else:\n",
        "    print(\"[INFO] -- Dataset empty, skipping training.\")"
      ],
      "metadata": {
        "id": "E5DUTdKNcqmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not dataset.empty:\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(range(1, NUM_EPOCHS + 1), train_losses_history, label='Training Loss', marker='o', linestyle='-')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(range(1, NUM_EPOCHS + 1), val_losses_history, label='Validation Loss', color='red', marker='x', linestyle='--')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Validation Loss Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(range(1, NUM_EPOCHS + 1), val_accuracies_history, label='Validation Accuracy', color='green', marker='s', linestyle='-.')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Validation Accuracy Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"[INFO] -- No training data to plot.\")\n"
      ],
      "metadata": {
        "id": "smDIw1ZOcuqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLbkavIh0Ojp"
      },
      "outputs": [],
      "source": [
        "if not dataset.empty:\n",
        "    # Sets model to evaluation mode\n",
        "    model.eval()\n",
        "    # Lists to hold predicted and true labels across all test batches\n",
        "    all_predicted_outputs_test = []\n",
        "    all_true_labels_test = []\n",
        "    # Disables gradient calculation for inference\n",
        "    with torch.no_grad():\n",
        "        for data_batch, labels_batch in test_loader:\n",
        "            # Move data to the same device as the model\n",
        "            data_batch = data_batch.to(device)\n",
        "            # Forward pass\n",
        "            outputs_logits = model(data_batch)\n",
        "            # Gets the index (class) with the highest score\n",
        "            _, predicted_batch_labels = torch.max(outputs_logits.data, 1)\n",
        "            # Stores predictions and corresponding true labels\n",
        "            all_predicted_outputs_test.extend(predicted_batch_labels.cpu().numpy())\n",
        "            all_true_labels_test.extend(labels_batch.cpu().numpy())\n",
        "\n",
        "    # Calculates overall test accuracy\n",
        "    final_test_accuracy = accuracy_score(all_true_labels_test, all_predicted_outputs_test)\n",
        "    # Computes the confusion matrix\n",
        "    test_confusion_mat = confusion_matrix(all_true_labels_test, all_predicted_outputs_test)\n",
        "    # Generates a detailed classification report including precision, recall, F1-score\n",
        "    test_classification_rep = classification_report(\n",
        "        all_true_labels_test, all_predicted_outputs_test, target_names=ACT_LABELS, zero_division=0\n",
        "    )\n",
        "\n",
        "    # Prints evaluation results\n",
        "    print(f\"\\n[INFO] -- CNN Model Evaluation on FINAL TEST Set:\")\n",
        "    print(f\"Overall Test Accuracy: {final_test_accuracy*100:.2f}%\")\n",
        "    print(\"\\nTest Set Classification Report:\")\n",
        "    print(test_classification_rep)\n",
        "    print(\"\\nTest Set Confusion Matrix:\")\n",
        "\n",
        "     # Plots the confusion matrix as a heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(test_confusion_mat, annot=True, fmt='d', cmap='BuPu',\n",
        "                xticklabels=ACT_LABELS, yticklabels=ACT_LABELS)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('CNN Confusion Matrix - Final Test Set')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"[INFO] -- Dataset empty, skipping final test set evaluation.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if dataset.empty:\n",
        "        print(\"\\nScript finished. NOTE: Dataset was empty, so most operations were skipped.\")\n",
        "    else:\n",
        "        print(\"\\nScript finished successfully.\")"
      ]
    }
  ]
}