{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Human Activity Recognition (HAR) with MotionSense Dataset (CWT + CNN Approach)"
      ],
      "metadata": {
        "id": "hBOaeVC2A-Gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "import pywt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "EpuGhpCscnaO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sets the device on which PyTorch tensors and models will be allocated\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Prints the selected device to confirm where computations will run\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "jSs15zYycoS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports the Colab-specific module to interact with Google Drive\n",
        "from google.colab import drive\n",
        "# Mounts the Google Drive to the Colab-environment\n",
        "drive.mount('/content/drive')\n",
        "# Defines a string variable DATA_PATH_PREFIX that stores the path to a specific folder in the Drive\n",
        "# Base path to access data files\n",
        "DATA_PATH_PREFIX = \"/content/drive/MyDrive/EEE4114F - ML/\""
      ],
      "metadata": {
        "id": "2aI6orpYctR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ds_infos():\n",
        "    \"\"\"\n",
        "    Read the file includes data subject information.\n",
        "    Returns:\n",
        "        A pandas DataFrame that contains information about data subjects' attributes\n",
        "    \"\"\"\n",
        "    # try block to handle errors\n",
        "    try:\n",
        "        # Attempts to load a CSV file named data_subjects_info.csv from the given DATA_PATH_PREFIX path\n",
        "        # dss holds the loaded DataFrame\n",
        "        dss = pd.read_csv(DATA_PATH_PREFIX + \"data_subjects_info.csv\")\n",
        "        # Prints a success message if the file loads without issues\n",
        "        print(\"[INFO] -- Data subjects' information is imported.\")\n",
        "    # Catches the error if the file is not found at the specific path\n",
        "    except FileNotFoundError:\n",
        "        # Prints an error message\n",
        "        print(f\"[ERROR] -- data_subjects_info.csv not found at {DATA_PATH_PREFIX + 'data_subjects_info.csv'}\")\n",
        "        print(\"Please ensure DATA_PATH_PREFIX is set correctly and the file exists.\")\n",
        "        # If the file is not found, it returns an empty DataFrame with a single column named 'code'\n",
        "        return pd.DataFrame(columns=['code'])\n",
        "    # if the file was successfully read, the loaded DataFrame (stored in dss) is returned\n",
        "    return dss"
      ],
      "metadata": {
        "id": "Y_Y7LfBfcvsh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_data_types(data_types=[\"userAcceleration\"]):\n",
        "    \"\"\"\n",
        "    Select the sensors and return a list of lists of column names.\n",
        "    Args:\n",
        "        data_types: A list of sensor data type from this list: [attitude, gravity, rotationRate, userAcceleration]\n",
        "    Returns:\n",
        "        A list of lists, where each inner list contains the column names for a sensor type (e.g., [t+\".x\",t+\".y\",t+\".z\"]).\n",
        "    \"\"\"\n",
        "    # Iintializes an empty list to store the results\n",
        "    dt_list_groups = []\n",
        "    # Loop through each sensor type specified in the data_types list\n",
        "    for t in data_types:\n",
        "        # Check if the sensor type is not attitude\n",
        "        # Yes: Construst a list of the 3-axis column names and append it to the result list\n",
        "        if t != \"attitude\":\n",
        "            dt_list_groups.append([t+\".x\", t+\".y\", t+\".z\"])\n",
        "        else:\n",
        "            # Attitude uses orientation angle\n",
        "            dt_list_groups.append([t+\".roll\", t+\".pitch\", t+\".yaw\"])\n",
        "    # Returns the list of column names\n",
        "    return dt_list_groups"
      ],
      "metadata": {
        "id": "KW-n2yPRcyRg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_time_series(dt_list_groups_config, act_labels_config, trial_codes_config_map, mode=\"raw\", labeled=True):\n",
        "    \"\"\"\n",
        "    Creates a time-series DataFrame from raw sensor data files.\n",
        "    Args:\n",
        "        dt_list_groups_config: List of lists of feature column names, grouped by sensor type.\n",
        "        act_labels_config: List of activity names.\n",
        "        trial_codes_config_map: Dictionary mapping activity names to lists of trial codes.\n",
        "        mode: \"raw\" (magnitude mode is not fully supported by this simplified version).\n",
        "        labeled: Boolean, True if activity labels should be included.\n",
        "    Returns:\n",
        "        A tuple: (full_dataset_df, feature_cols_flat_list)\n",
        "            full_dataset_df: Pandas DataFrame with all time-series data and labels.\n",
        "            feature_cols_flat_list: Flat list of all feature column names.\n",
        "    \"\"\"\n",
        "    # Flattens the list of lists into a single list of feature column names\n",
        "    feature_cols_flat_list = [col for group in dt_list_groups_config for col in group]\n",
        "    num_feature_cols = len(feature_cols_flat_list)\n",
        "\n",
        "    # Initializes the column names for the final DataFrame\n",
        "    column_names_for_df = feature_cols_flat_list[:]\n",
        "    # Adds \"act\" column if activity labes are included\n",
        "    if labeled:\n",
        "        column_names_for_df.append(\"act\")\n",
        "\n",
        "    # Loads subject metada\n",
        "    all_trial_dfs = []\n",
        "    ds_list = get_ds_infos()\n",
        "    # Check if subject info is missing\n",
        "    # Yes: Prints an error message, aborts and returns an empty DataFrame\n",
        "    if ds_list.empty and 'code' not in ds_list.columns:\n",
        "        print(\"[ERROR] -- Cannot proceed without subject information.\")\n",
        "        return pd.DataFrame(columns=column_names_for_df), feature_cols_flat_list\n",
        "\n",
        "\n",
        "    print(\"[INFO] -- Creating Time-Series\")\n",
        "    # Loops hrough all subjects and trials\n",
        "    for sub_id in ds_list[\"code\"]:\n",
        "        for act_id, act_name in enumerate(act_labels_config):\n",
        "            # Checks if no trials are configure\n",
        "            # Yes:  Skips the activity\n",
        "            if act_name not in trial_codes_config_map:\n",
        "                print(f\"[WARNING] -- No trial codes found for activity: {act_name}. Skipping.\")\n",
        "                continue\n",
        "            for trial_code in trial_codes_config_map[act_name]:\n",
        "                # Constructs the full file path for each subject's trial\n",
        "                fname = f'{DATA_PATH_PREFIX}A_DeviceMotion_data/A_DeviceMotion_data/{act_name}_{trial_code}/sub_{int(sub_id)}.csv'\n",
        "                # try block to handle errors\n",
        "                try:\n",
        "                    # Attampts to read the CSV file\n",
        "                    raw_data_per_trial = pd.read_csv(fname)\n",
        "                # Catches the error if the file is not found\n",
        "                except FileNotFoundError:\n",
        "                    # Prints a warninng message\n",
        "                    print(f\"[WARNING] -- File not found: {fname}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Drops unwanted index columns automatically added by pandas during saving\n",
        "                raw_data_per_trial = raw_data_per_trial.drop(['Unnamed: 0'], axis=1, errors='ignore')\n",
        "\n",
        "                # Extracts only the relevant feature columns for this trial\n",
        "                current_trial_features = raw_data_per_trial[feature_cols_flat_list].values\n",
        "\n",
        "               # Checks if labeled\n",
        "               # Creates an array of labels with the same numbeer as the featires\n",
        "                if labeled:\n",
        "                    labels_for_this_trial = np.full((len(raw_data_per_trial), 1), act_id)\n",
        "                    trial_data_np = np.concatenate((current_trial_features, labels_for_this_trial), axis=1)\n",
        "                else:\n",
        "                    trial_data_np = current_trial_features\n",
        "                # Converts the current trial into a DataFrame and apends to the list of all trials\n",
        "                all_trial_dfs.append(pd.DataFrame(data=trial_data_np, columns=column_names_for_df))\n",
        "\n",
        "    # Checks if no trails loaded successfully\n",
        "    # Yes: Returns an empty DataFrame\n",
        "    if not all_trial_dfs:\n",
        "        print(\"[ERROR] -- No data successfully loaded. Please check file paths, data structure, and configurations.\")\n",
        "        return pd.DataFrame(columns=column_names_for_df), feature_cols_flat_list\n",
        "\n",
        "    print(f\"[INFO] -- Concatenating {len(all_trial_dfs)} individual trial DataFrames.\")\n",
        "    # Concatenates all DataFrames into one final DataFrame\n",
        "    full_dataset_df = pd.concat(all_trial_dfs, ignore_index=True)\n",
        "    # Returns the complete time-series dataset (full_dataset_df) and a list of column names for model input (feature_cols_flat_list)\n",
        "    return full_dataset_df, feature_cols_flat_list"
      ],
      "metadata": {
        "id": "9IJ8XAf_c5ni"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_cwt_to_signal(signal_data, scales=None, wavelet='morlet', sampling_rate=50):\n",
        "    \"\"\"\n",
        "    Apply Continuous Wavelet Transform to a 1D signal.\n",
        "    Args:\n",
        "        signal_data: 1D numpy array representing the signal\n",
        "        scales: Array of scales for CWT. If None, auto-generated\n",
        "        wavelet: Type of wavelet to use (default: 'morlet')\n",
        "        sampling_rate: Sampling rate of the signal in Hz\n",
        "    Returns:\n",
        "        cwt_coeffs: 2D array of CWT coefficients (scales x time)\n",
        "        frequencies: Corresponding frequencies for each scale\n",
        "    \"\"\"\n",
        "    if scales is None:\n",
        "        frequencies = np.logspace(np.log10(1), np.log10(25), 30)\n",
        "        scales = pywt.frequency2scale(wavelet, frequencies) * sampling_rate\n",
        "\n",
        "    # Compute CWT coefficients\n",
        "    cwt_coeffs, frequencies = pywt.cwt(signal_data, scales, wavelet, sampling_period=1/sampling_rate)\n",
        "\n",
        "    # Extracts the magnitude of complex coefficients\n",
        "    cwt_coeffs = np.abs(cwt_coeffs)\n",
        "    # Returns:\n",
        "    #   cwt_coeff: Magnitude of the wavelet coefficients.\n",
        "    #   frequencies: Frequencies associated with each scale used\n",
        "    return cwt_coeffs, frequencies"
      ],
      "metadata": {
        "id": "FWhs-Bdrc81m"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cwt_windows_from_df(data_df, feature_cols, label_col, window_size, stride,\n",
        "                              scales=None, wavelet='morlet', sampling_rate=50):\n",
        "    \"\"\"\n",
        "    Creates windowed CWT data from a DataFrame.\n",
        "    Args:\n",
        "        data_df: Input DataFrame with features and labels.\n",
        "        feature_cols: List of feature column names.\n",
        "        label_col: Name of the label column.\n",
        "        window_size: Size of each window.\n",
        "        stride: Stride between windows.\n",
        "        scales: CWT scales (if None, auto-generated)\n",
        "        wavelet: Wavelet type for CWT\n",
        "        sampling_rate: Sampling rate in Hz\n",
        "    Returns:\n",
        "        A tuple (np.array(X_cwt_windows), np.array(Y_labels)).\n",
        "        X_cwt_windows shape: (num_windows, num_features, num_scales, window_size)\n",
        "        Y_labels shape: (num_windows,)\n",
        "    \"\"\"\n",
        "    X_cwt_windows_list = []\n",
        "    Y_labels_list = []\n",
        "\n",
        "    feature_data_np = data_df[feature_cols].values\n",
        "    label_data_np = data_df[label_col].values\n",
        "\n",
        "    print(f\"[INFO] -- Applying CWT to {len(feature_cols)} features...\")\n",
        "\n",
        "    # Generate scales if not provided\n",
        "    if scales is None:\n",
        "        frequencies = np.logspace(np.log10(1), np.log10(25), 30)\n",
        "        scales = pywt.frequency2scale(wavelet, frequencies) * sampling_rate\n",
        "\n",
        "    num_scales = len(scales)\n",
        "    print(f\"[INFO] -- Using {num_scales} CWT scales\")\n",
        "\n",
        "    for i in range(0, len(feature_data_np) - window_size + 1, stride):\n",
        "        window_features = feature_data_np[i : i + window_size]  # Shape: (window_size, num_features)\n",
        "        window_labels_raw = label_data_np[i : i + window_size]\n",
        "\n",
        "        # Apply CWT to each feature in the window\n",
        "        cwt_features_list = []\n",
        "        for feature_idx in range(len(feature_cols)):\n",
        "            signal = window_features[:, feature_idx]\n",
        "            cwt_coeffs, _ = apply_cwt_to_signal(signal, scales, wavelet, sampling_rate)\n",
        "            cwt_features_list.append(cwt_coeffs)  # Shape: (num_scales, window_size)\n",
        "\n",
        "        # Stack CWT features: (num_features, num_scales, window_size)\n",
        "        cwt_window = np.stack(cwt_features_list, axis=0)\n",
        "        X_cwt_windows_list.append(cwt_window)\n",
        "\n",
        "        # Use mode of labels in the window as the window's label\n",
        "        label_counts = np.bincount(window_labels_raw.astype(int))\n",
        "        mode_label = np.argmax(label_counts)\n",
        "        Y_labels_list.append(mode_label)\n",
        "\n",
        "        if (len(X_cwt_windows_list) % 100) == 0:\n",
        "            print(f\"[INFO] -- Processed {len(X_cwt_windows_list)} windows...\")\n",
        "\n",
        "    return np.array(X_cwt_windows_list), np.array(Y_labels_list)"
      ],
      "metadata": {
        "id": "h5XiD7yVfEdM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines a list of activity labels\n",
        "ACT_LABELS = [\"dws\",\"ups\", \"wlk\", \"jog\", \"std\", \"sit\"]\n",
        "# Creates a mapping between each activity and its corresponding trial codes\n",
        "TRIAL_CODES = {\n",
        "    ACT_LABELS[0]:[1,2,11], #dws: trials 1, 2, 11\n",
        "    ACT_LABELS[1]:[3,4,12], #ups: trials 3, 4, 12\n",
        "    ACT_LABELS[2]:[7,8,15], #wlk: trials 7, 8, 15\n",
        "    ACT_LABELS[3]:[9,16],   #jog: trials 9, 16\n",
        "    ACT_LABELS[4]:[6,14],   #std: trials 6, 14\n",
        "    ACT_LABELS[5]:[5,13]    #sit: trials 5, 13\n",
        "}\n",
        "# Specifies which sensor data types to include from each CSV file\n",
        "# Available options: \"attitude\", \"gravity\", \"rotationRate\", \"userAcceleration\"\n",
        "SELECTED_SENSOR_DATA_TYPES = [\"userAcceleration\", \"rotationRate\"]\n",
        "# Prints the current configuration\n",
        "print(f\"[INFO] -- Selected sensor data types: {SELECTED_SENSOR_DATA_TYPES}\")\n",
        "print(f\"[INFO] -- Selected activities: {ACT_LABELS}\")"
      ],
      "metadata": {
        "id": "x_ADhq14fR9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calls the set_data_types() function with the selected sensor types ([\"userAcceleration\", \"rotationRate\"])\n",
        "# Returns a list of feature column names grouped by sensor type\n",
        "dt_list_feature_groups = set_data_types(SELECTED_SENSOR_DATA_TYPES)\n",
        "# Runs the data loading pipleline:\n",
        "# - Loads CSVs for each subject/activity/trial\n",
        "# - Extracts the relevant sensor feature\n",
        "# - Attaches activity labels\n",
        "# Returns:\n",
        "# - dataset: contains all time-series data across subjects/trials\n",
        "# - feature_columns_list: A list of selected feature column names\n",
        "dataset, feature_columns_list = create_time_series(dt_list_feature_groups, ACT_LABELS, TRIAL_CODES, mode=\"raw\", labeled=True)\n",
        "\n",
        "# Checks if no data was successfully loaded\n",
        "# Yes: Prints an error message and termitnaes executio\n",
        "if dataset.empty:\n",
        "    print(\"[STOP] -- Dataset is empty. Halting execution. Check file paths and data availability.\")\n",
        "    exit()\n",
        "# Data Loaded Successfully\n",
        "else:\n",
        "    # Prints the shae of the DataFrame\n",
        "    print(f\"[INFO] -- Shape of raw time-Series dataset: {dataset.shape}\")\n",
        "    # Promts the first few rows of data using head()\n",
        "    print(dataset.head())\n",
        "    # Number of input features per time step\n",
        "    NUM_FEATURES = len(feature_columns_list)\n",
        "    # Number of unique activity labels\n",
        "    NUM_CLASSES = len(ACT_LABELS)"
      ],
      "metadata": {
        "id": "O7N6ZPQrfbx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of samples in each sliding window segment.\n",
        "WINDOW_SIZE = 128\n",
        "# Step size for moving the window across the time series.\n",
        "STRIDE = 64\n",
        "SAMPLING_RATE = 50\n",
        "CWT_SCALES = None\n",
        "WAVELET_TYPE = 'morl'\n",
        "\n",
        "# Sampling frequency of the sensor data in Hz (samples per second).\n",
        "SAMPLING_RATE = 50\n",
        "\n",
        "# Scales to be used in Continuous Wavelet Transform (CWT).\n",
        "CWT_SCALES = None\n",
        "\n",
        "# Type of wavelet to be used in CWT → 'morl': Morlet wavelet\n",
        "WAVELET_TYPE = 'morl'\n"
      ],
      "metadata": {
        "id": "DE5E2n5gfn_j"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a StandardScaler to normalize the feature data.\n",
        "scaler = StandardScaler()\n",
        "# Create a copy of the original dataset\n",
        "scaled_dataset = dataset.copy()\n",
        "# Apples the scaler only to the selected feature columns.\n",
        "scaled_dataset[feature_columns_list] = scaler.fit_transform(dataset[feature_columns_list])\n",
        "print(\"[INFO] -- Features scaled using StandardScaler.\")\n",
        "# Prints confirmation output that feature scaling is complete.\n",
        "print(\"[INFO] -- Features scaled using StandardScaler.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7toGhOjfxr2",
        "outputId": "e10cdeb3-2d82-415b-9976-2c1d530855d2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] -- Features scaled using StandardScaler.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates CWT windows from the scaled dataset\n",
        "# Applies Continuous Wavelet Transform to each window of the time-series data\n",
        "print(\"[INFO] -- Starting CWT transformation and windowing...\")\n",
        "X_cwt_windowed, Y_windowed = create_cwt_windows_from_df(\n",
        "    scaled_dataset, feature_columns_list, 'act',\n",
        "    WINDOW_SIZE, STRIDE, CWT_SCALES, WAVELET_TYPE, SAMPLING_RATE\n",
        ")\n",
        "\n",
        "# Prints shapes of resulting CWT feature array and corresponding labels\n",
        "print(f\"[INFO] -- Shape of CWT windowed X: {X_cwt_windowed.shape}\")\n",
        "print(f\"[INFO] -- Shape of windowed Y: {Y_windowed.shape}\")\n",
        "\n",
        "# Extracts dimensions from the CWT-transformed windowed data\n",
        "num_windows, num_features, num_scales, window_size = X_cwt_windowed.shape\n",
        "\n",
        "# Reshape the 4D CWT array to 3D for compatibility with CNN input:\n",
        "X_cwt_reshaped = X_cwt_windowed.reshape(num_windows, num_features * num_scales, window_size)\n",
        "\n",
        "\n",
        "# Prints final shape of reshaped data\n",
        "print(f\"[INFO] -- Reshaped CWT data for CNN: {X_cwt_reshaped.shape}\")"
      ],
      "metadata": {
        "id": "NS2RUD7Nf6mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the CWT-transformed and reshaped dataset into 80% for training and 20% for test sets.\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X_cwt_reshaped, Y_windowed, test_size=0.2, random_state=42, stratify=Y_windowed\n",
        ")\n",
        "\n",
        "# Prints the shapes pf the resulting train and test sets\n",
        "print(f\"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, Y_test shape: {Y_test.shape}\")"
      ],
      "metadata": {
        "id": "730_5C0bgBlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MotionSensePyTorchDataset(Dataset):\n",
        "    def __init__(self, X_data, Y_data):\n",
        "        # Converts input data to float32 tensors\n",
        "        self.X = torch.tensor(X_data, dtype=torch.float32)\n",
        "        # Converts labels to long tensors (required by CrossEntropyLoss)\n",
        "        self.Y = torch.tensor(Y_data, dtype=torch.long)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Returns a single sample and its label at the given index\n",
        "        return self.X[index], self.Y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the total number of samples in the dataset\n",
        "        return len(self.X)\n",
        "\n",
        "# Instantiates PyTorch Datasets for training and testing sets\n",
        "train_torch_dataset = MotionSensePyTorchDataset(X_train, Y_train)\n",
        "test_torch_dataset = MotionSensePyTorchDataset(X_test, Y_test)\n",
        "\n",
        "# Sets batch size for data loading\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Creates DataLoaders to load data in batches\n",
        "train_loader = DataLoader(train_torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_torch_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Prints confirmation output that PyTorch DataLoaders have been created\n",
        "print(\"[INFO] -- PyTorch DataLoaders created.\")"
      ],
      "metadata": {
        "id": "JYyrMI4hgLRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CWT_CNN_HAR_Model(nn.Module):\n",
        "    def __init__(self, num_input_channels, num_activity_classes, sequence_length):\n",
        "        super(CWT_CNN_HAR_Model, self).__init__()\n",
        "\n",
        "        # First convolutional layer\n",
        "        self.conv_layer1 = nn.Conv1d(in_channels=num_input_channels, out_channels=128, kernel_size=9, padding='same')\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "\n",
        "        # Second convoluional layer\n",
        "        self.conv_layer2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=7, padding='same')\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "\n",
        "        # Third convolutional layer\n",
        "        self.conv_layer3 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=5, padding='same')\n",
        "        self.bn3 = nn.BatchNorm1d(512)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.maxpool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.dropout3 = nn.Dropout(0.3)\n",
        "\n",
        "        # Computes the size of the flattened feature vector after the convolutions\n",
        "        flattened_output_size = 512 * (sequence_length // 8)\n",
        "\n",
        "        # Fully connected layers for classification\n",
        "        self.flatten_layer = nn.Flatten()\n",
        "        self.fc_layer1 = nn.Linear(flattened_output_size, 256)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.dropout4 = nn.Dropout(0.5)\n",
        "\n",
        "        # Final output layer mapping to activity classes\n",
        "        self.fc_layer2 = nn.Linear(256, 128)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.dropout5 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc_layer3 = nn.Linear(128, num_activity_classes)\n",
        "\n",
        "    def forward(self, x_input):\n",
        "         # Forward pass through the networ\n",
        "        x = self.dropout1(self.maxpool1(self.relu1(self.bn1(self.conv_layer1(x_input)))))\n",
        "        x = self.dropout2(self.maxpool2(self.relu2(self.bn2(self.conv_layer2(x)))))\n",
        "        x = self.dropout3(self.maxpool3(self.relu3(self.bn3(self.conv_layer3(x)))))\n",
        "\n",
        "        x = self.flatten_layer(x)\n",
        "        x = self.dropout4(self.relu4(self.fc_layer1(x)))\n",
        "        x = self.dropout5(self.relu5(self.fc_layer2(x)))\n",
        "        output_logits = self.fc_layer3(x)\n",
        "        return output_logits"
      ],
      "metadata": {
        "id": "Vzz-Zi77gXIf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculates the number of input channels after CWT transformation\n",
        "NUM_CWT_CHANNELS = num_features * num_scales\n",
        "\n",
        "# Instantiates the model and moves the model to approriate device\n",
        "model = CWT_CNN_HAR_Model(\n",
        "    num_input_channels=NUM_CWT_CHANNELS,\n",
        "    num_activity_classes=NUM_CLASSES,\n",
        "    sequence_length=WINDOW_SIZE\n",
        ").to(device)\n",
        "\n",
        "# Prints model summary and configuration info\n",
        "print(\"[INFO] -- CWT-CNN Model instantiated:\")\n",
        "print(f\"[INFO] -- Input channels (features × CWT scales): {NUM_CWT_CHANNELS}\")\n",
        "print(f\"[INFO] -- Number of classes: {NUM_CLASSES}\")\n",
        "print(f\"[INFO] -- Sequence length: {WINDOW_SIZE}\")\n",
        "print(model)"
      ],
      "metadata": {
        "id": "R7n32rC8gj5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines loss function and optimizer\n",
        "loss_criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Defines number of epochs for training\n",
        "NUM_EPOCHS = 50\n",
        "print(f\"[INFO] -- Starting training for {NUM_EPOCHS} epochs...\")\n",
        "\n",
        "# Lists to store training loss and test accuracy for each epoch\n",
        "train_losses_history = []\n",
        "test_accuracies_history = []\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "   # Sets the model to training mode\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    # Loops through training data in batches\n",
        "    for batch_idx, (data_batch, labels_batch) in enumerate(train_loader):\n",
        "        # Move batch data to the approriate device\n",
        "        data_batch, labels_batch = data_batch.to(device), labels_batch.to(device)\n",
        "        # Clears previous gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        outputs_logits = model(data_batch)\n",
        "        # Calculate loss\n",
        "        loss = loss_criterion(outputs_logits, labels_batch)\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        # Updates model parameters\n",
        "        optimizer.step()\n",
        "        # Accumulated training loss\n",
        "        total_train_loss += loss.item()\n",
        "        # Prints training progress every 50 batches\n",
        "        if (batch_idx + 1) % 50 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Batch [{batch_idx+1}/{len(train_loader)}], Batch Loss: {loss.item():.4f}\")\n",
        "    # Computes average training loss for the epoch and store it\n",
        "    avg_epoch_train_loss = total_train_loss / len(train_loader)\n",
        "    train_losses_history.append(avg_epoch_train_loss)\n",
        "\n",
        "    # Steps the learning rate scheduler\n",
        "    scheduler.step()\n",
        "    # Gets the current learning rate\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "    # Evaluation phase on test set\n",
        "    # Sets model to evaluation mode\n",
        "    model.eval()\n",
        "    total_test_correct = 0\n",
        "    total_test_samples = 0\n",
        "    # Disables gradient calculation\n",
        "    with torch.no_grad():\n",
        "        for data_batch, labels_batch in test_loader:\n",
        "            data_batch, labels_batch = data_batch.to(device), labels_batch.to(device)\n",
        "            outputs_logits = model(data_batch)\n",
        "            # Gets predicted class labels\n",
        "            _, predicted_labels = torch.max(outputs_logits.data, 1)\n",
        "            total_test_samples += labels_batch.size(0)\n",
        "            total_test_correct += (predicted_labels == labels_batch).sum().item()\n",
        "    # Calculates accuracy for the current epoch and stores it\n",
        "    epoch_test_accuracy = 100 * total_test_correct / total_test_samples\n",
        "    test_accuracies_history.append(epoch_test_accuracy)\n",
        "\n",
        "    # Prints epoch summary\n",
        "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {avg_epoch_train_loss:.4f}, Test Acc: {epoch_test_accuracy:.2f}%, LR: {current_lr:.6f}\")\n",
        "# Training complete\n",
        "print(\"[INFO] -- Training Finished.\")"
      ],
      "metadata": {
        "id": "3mizjaWYIf0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, NUM_EPOCHS + 1), train_losses_history, label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, NUM_EPOCHS + 1), test_accuracies_history, label='Test Accuracy', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Test Accuracy Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FE24DDE0gxC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmAHaCppcT3i"
      },
      "outputs": [],
      "source": [
        "# Sets model to evaluation mode\n",
        "model.eval()\n",
        "# Lists to store all predicted labels and true labels\n",
        "all_predicted_outputs = []\n",
        "all_true_labels_eval = []\n",
        "\n",
        "# Disable gradients computation for evaluation\n",
        "with torch.no_grad():\n",
        "    # Iterates through test set in batches\n",
        "    for data_batch, labels_batch in test_loader:\n",
        "        # Moves input data to the device (GPU or CPU)\n",
        "        data_batch = data_batch.to(device)\n",
        "        # Forward pass\n",
        "        outputs_logits = model(data_batch)\n",
        "         # Gets predicted class by taking the argmax of the output logits\n",
        "        _, predicted_batch_labels = torch.max(outputs_logits.data, 1)\n",
        "        # Stores predictions and corresponding true labels\n",
        "        all_predicted_outputs.extend(predicted_batch_labels.cpu().numpy())\n",
        "        all_true_labels_eval.extend(labels_batch.cpu().numpy()) # labels_batch are already on CPU from DataLoader\n",
        "\n",
        "# Computes accuracy of final model on test set\n",
        "final_model_accuracy = accuracy_score(all_true_labels_eval, all_predicted_outputs)\n",
        "# Computes the confusion matrix\n",
        "confusion_mat = confusion_matrix(all_true_labels_eval, all_predicted_outputs)\n",
        "# Generates a detailed classification report including precision, recall, F1-score\n",
        "classification_rep = classification_report(\n",
        "    all_true_labels_eval, all_predicted_outputs, target_names=ACT_LABELS, zero_division=0\n",
        ")\n",
        " # Prints evaluation results\n",
        "print(f\"\\n[INFO] -- Final CWT-CNN Model Evaluation on Test Set:\")\n",
        "print(f\"Overall Accuracy: {final_model_accuracy*100:.2f}%\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_rep)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "\n",
        " # Plots the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=ACT_LABELS, yticklabels=ACT_LABELS)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('CWT-CNN Confusion Matrix - Test Set')\n",
        "plt.show()"
      ]
    }
  ]
}